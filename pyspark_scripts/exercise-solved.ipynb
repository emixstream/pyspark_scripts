{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Data Pipeline with Apache Spark - Demo \n",
    "\n",
    "In this demo we will see together how to leverage [Apache Spark](https://spark.apache.org/) via Python ([PySpark](http://spark.apache.org/docs/latest/api/python/)) to process data from different sources representing a Data Lake. \n",
    "For the demo purpose we will use data stored in:\n",
    "\n",
    "* __MySQL__ - RDBMS database\n",
    "* __MongoDB__ - NoSQL database\n",
    "* __Parquet__ Files - [Apache Parquet](https://parquet.apache.org/) is a columnar data format often used in [Apache Hadoop](https://hadoop.apache.org/) environments, particularly suitable for analytics\n",
    "\n",
    "\n",
    "The basic idea is to design and implement a Big Data Pipeline consisting of several **Jobs** and **Tasks**. \n",
    "\n",
    "* Job: a complete data tranformation activity, from reading data from a source to saving them somewhere\n",
    "* Task: a single step of a job\n",
    "___\n",
    "\n",
    "To start the demo, run:\n",
    "\n",
    "`docker-compose -f ./docker-compose-full.yml up -d`\n",
    " \n",
    "To stop the demo, run:\n",
    "\n",
    "`docker-compose -f ./docker-compose-full.yml down`\n",
    "\n",
    "Docker will build up an ecosystem with:\n",
    "\n",
    "* **MongoDB** with a populated database\n",
    "* **MySQL** with a populated database\n",
    "* **Apache Spark** deployed in Standalone Mode\n",
    "* **Jupyter** enabled to work with Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "In the first cell we have to instantiate the __SparkSession__ object. Via the SparkSession we can read, manipulate, and store data from different data sources using both RDD and DataFrame API. \n",
    "\n",
    "> Only one SparkSession object can be contained in a Spark-powered program. The SparkSession creates and handles the DAG and interacts with the exectutors to execute it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://root:example@mongo/test.coll?authSource=admin\") \\\n",
    ".config(\"spark.mongodb.output.uri\", \"mongodb://root:example@mongo/test.coll?authSource=admin\") \\\n",
    ".config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.17,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1') \\\n",
    ".getOrCreate()\n",
    "ss.version\n",
    "# Spark version 2.4.4 uses Scala 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [builder pattern](https://en.wikipedia.org/wiki/Builder_pattern) is used to create and initialize the SparkSession. Notice that we used the *config* method to store metadata as the MondoDB input and output uris and a list of java packages (fully specified in [Apache Maven](https://maven.apache.org/) format). \n",
    "\n",
    "> Note that Spark checks if the listed jar packages are available at executor level, otherwise it downloads them.  \n",
    "\n",
    "Other metadata (hostname, username, password, etc.) are to be expressed as python variable. Next cell reports MySQL connection parameters. In the following the **jdbcUrl** is used for DataFrame creation from MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"mysql\"\n",
    "jdbcDatabase = \"esame\"\n",
    "username = \"root\"\n",
    "password = \"example\"\n",
    "jdbcPort = 3306\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Total sales per film category\n",
    "\n",
    "List the **total sales per film category** considering only the sales referred to rented movies.\n",
    "Measure the query execution time (you might want to use the python 'time' library)\n",
    "\n",
    "You may need to use the following tables: \n",
    "\n",
    "1. category\n",
    "2. film_category \n",
    "3. inventory \n",
    "3. payment \n",
    "4. rental\n",
    "\n",
    "![EER Diagram](figures/mysql_table1.png)\n",
    "\n",
    "\n",
    "The result must have __two columns__:\n",
    "\n",
    "1. the film_category\n",
    "2. the total_sales\n",
    "\n",
    "and has to be **sorted in descending order** with respect to the **total_sales**.\n",
    "\n",
    "> For performance reasons it is recommended to write a SQL query rather than import the data in different spark dataframes and use spark to join them. It is better because Spark is able to communicate with MySQL and push down filter and join operations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet presents how to create a dataframe from MySQL. \n",
    "\n",
    "Note that it is necessary to provide:\n",
    "\n",
    "1. The protocol to use (jdbc, driver)\n",
    "2. The connection string (url)\n",
    "3. The query MySQL has to execute to export data to Spark\n",
    "\n",
    "Moreover, note that the dataframe API is lazy evaluated, it needs an **action**. In the code snippet below:\n",
    "\n",
    "* load() is not an action, the dataframe is therefore only defined but not created\n",
    "* show() is an action, the dataframe is created here and the results are returned to the main program (driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   category|total_sales|\n",
      "+-----------+-----------+\n",
      "|     Sports|    5314.21|\n",
      "|     Sci-Fi|    4756.98|\n",
      "|  Animation|    4656.30|\n",
      "|      Drama|    4587.39|\n",
      "|     Comedy|    4383.58|\n",
      "|     Action|    4375.85|\n",
      "|        New|    4351.62|\n",
      "|      Games|    4281.33|\n",
      "|    Foreign|    4270.67|\n",
      "|     Family|    4226.07|\n",
      "|Documentary|    4217.52|\n",
      "|     Horror|    3722.54|\n",
      "|   Children|    3655.55|\n",
      "|   Classics|    3639.59|\n",
      "|     Travel|    3549.64|\n",
      "|      Music|    3417.72|\n",
      "+-----------+-----------+\n",
      "\n",
      "Time:  135.85194158554077\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "import time\n",
    "\n",
    "query1 =  '''\n",
    "    SELECT \n",
    "        c.name AS category, \n",
    "        SUM(p.amount) AS total_sales\n",
    "    FROM\n",
    "        payment p\n",
    "        JOIN rental r ON p.rental_id = r.rental_id\n",
    "        JOIN inventory i ON r.inventory_id = i.inventory_id\n",
    "        JOIN film f ON i.film_id = f.film_id\n",
    "        JOIN film_category fc ON f.film_id = fc.film_id\n",
    "        JOIN category c ON fc.category_id = c.category_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_sales DESC\n",
    "    '''\n",
    "\n",
    "salesCat = ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query1)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "salesCat.show() # this is an action, the dataframe is created only at this point.\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimizing data loading with indexes\n",
    "\n",
    "Indexes are extremely important for speeding up analytical processes, particularly when JOIN operations must be performed. It is easy to implement indexes in RDBMS and NoSQL systems, a little less easy when dealing with large files in HDFS. \n",
    "\n",
    "**Optimize** the query created in the previous cell by entering the appropriate indexes in the mysql database. \n",
    "\n",
    "**Report** as comments the sql statements used to create the indexes\n",
    "\n",
    "**Re-execute** the query (reating the dataframe df2) and measure the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   category|total_sales|\n",
      "+-----------+-----------+\n",
      "|     Sports|    5314.21|\n",
      "|     Sci-Fi|    4756.98|\n",
      "|  Animation|    4656.30|\n",
      "|      Drama|    4587.39|\n",
      "|     Comedy|    4383.58|\n",
      "|     Action|    4375.85|\n",
      "|        New|    4351.62|\n",
      "|      Games|    4281.33|\n",
      "|    Foreign|    4270.67|\n",
      "|     Family|    4226.07|\n",
      "|Documentary|    4217.52|\n",
      "|     Horror|    3722.54|\n",
      "|   Children|    3655.55|\n",
      "|   Classics|    3639.59|\n",
      "|     Travel|    3549.64|\n",
      "|      Music|    3417.72|\n",
      "+-----------+-----------+\n",
      "\n",
      "Time:  0.3575015068054199\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "# ALTER TABLE `esame`.`inventory` CHANGE COLUMN `inventory_id` `inventory_id` MEDIUMINT(8) UNSIGNED NOT NULL AUTO_INCREMENT , ADD PRIMARY KEY (`inventory_id`);\n",
    "# ALTER TABLE `esame`.`rental` CHANGE COLUMN `rental_id` `rental_id` INT(11) NOT NULL AUTO_INCREMENT ,ADD PRIMARY KEY (`rental_id`);\n",
    "# ALTER TABLE `esame`.`inventory` ADD INDEX `store_id_idx` (`store_id` ASC) VISIBLE;\n",
    "# ALTER TABLE `esame`.`payment` ADD INDEX `rental_id_idx` (`rental_id` ASC) VISIBLE;\n",
    "# ALTER TABLE `esame`.`rental` ADD INDEX `inventory_id_idx` (`inventory_id` ASC) VISIBLE;\n",
    "# ALTER TABLE `esame`.`inventory` ADD INDEX `index2` (`film_id` ASC) VISIBLE;\n",
    "# ALTER TABLE `esame`.`film_category` ADD INDEX `index1` (`film_id` ASC) VISIBLE;\n",
    "# ALTER TABLE `esame`.`category` CHANGE COLUMN `category_id` `category_id` TINYINT(3) UNSIGNED NOT NULL AUTO_INCREMENT ,ADD PRIMARY KEY (`category_id`);\n",
    "# ALTER TABLE `esame`.`film_category` ADD PRIMARY KEY (`category_id`, `film_id`);\n",
    "\n",
    "# To drop the indexes\n",
    "# ALTER TABLE `esame`.`inventory` MODIFY inventory_id INT NOT NULL;\n",
    "# ALTER TABLE `esame`.`inventory` DROP PRIMARY KEY;\n",
    "# ALTER TABLE `esame`.`inventory` DROP INDEX `store_id_idx`;\n",
    "# ALTER TABLE `esame`.`rental` MODIFY rental_id INT NOT NULL;\n",
    "# ALTER TABLE `esame`.`rental` DROP PRIMARY KEY;\n",
    "# ALTER TABLE `esame`.`payment` DROP INDEX `rental_id_idx`;\n",
    "# ALTER TABLE `esame`.`rental` DROP INDEX `inventory_id_idx`;\n",
    "# ALTER TABLE `esame`.`inventory` DROP INDEX `index2`;\n",
    "# ALTER TABLE `esame`.`film_category` DROP INDEX `index1` ;\n",
    "# ALTER TABLE `esame`.`category` MODIFY category_id INT NOT NULL;\n",
    "# ALTER TABLE `esame`.`category` DROP PRIMARY KEY;\n",
    "# ALTER TABLE `esame`.`film_category` DROP PRIMARY KEY ;\n",
    "\n",
    "start = time.time()\n",
    "salesCat.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing data loading with views\n",
    "\n",
    "Views are a great way to simplify the definition of a data pipeline because it defines tasks to be executed at the database level, in some cases it also allows to improve the overall query performance. \n",
    "\n",
    "In the big data world, a view can be implemented as a **batch process**, for example implemented through [Apache HIVE](https://hive.apache.org/), which makes partially pre-processed data available. \n",
    "This is very useful when you want to crate a data pipeline for production, a bit less so when you want to implement exploratory actions.\n",
    "\n",
    "1. **Create** a view called \"total_sales\" from the query implemented in the previous cells.\n",
    "2. **Report** here the sql statement used to create the view\n",
    "3. **Load** in a spark dataframe all the rows of the view and show them\n",
    "4. **Measure** the data loading time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   category|total_sales|\n",
      "+-----------+-----------+\n",
      "|     Sports|    5314.21|\n",
      "|     Sci-Fi|    4756.98|\n",
      "|  Animation|    4656.30|\n",
      "|      Drama|    4587.39|\n",
      "|     Comedy|    4383.58|\n",
      "|     Action|    4375.85|\n",
      "|        New|    4351.62|\n",
      "|      Games|    4281.33|\n",
      "|    Foreign|    4270.67|\n",
      "|     Family|    4226.07|\n",
      "|Documentary|    4217.52|\n",
      "|     Horror|    3722.54|\n",
      "|   Children|    3655.55|\n",
      "|   Classics|    3639.59|\n",
      "|     Travel|    3549.64|\n",
      "|      Music|    3417.72|\n",
      "+-----------+-----------+\n",
      "\n",
      "Time:  0.3496370315551758\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "# put here the sql statement for the view creation\n",
    "\n",
    "#  CREATE VIEW total_sales AS\n",
    "#     SELECT \n",
    "#         c.name AS category, SUM(p.amount) AS total_sales\n",
    "#     FROM\n",
    "#         payment p\n",
    "#             JOIN\n",
    "#         rental r ON p.rental_id = r.rental_id\n",
    "#             JOIN\n",
    "#         inventory i ON r.inventory_id = i.inventory_id\n",
    "#             JOIN\n",
    "#         film f ON i.film_id = f.film_id\n",
    "#             JOIN\n",
    "#         film_category fc ON f.film_id = fc.film_id\n",
    "#             JOIN\n",
    "#         category c ON fc.category_id = c.category_id\n",
    "#     GROUP BY c.name\n",
    "#     ORDER BY total_sales DESC\n",
    "\n",
    "import time\n",
    "\n",
    "query2 = '''select * from total_sales'''\n",
    "\n",
    "dfview = ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query2)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "start = time.time()\n",
    "dfview.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Films per category\n",
    "\n",
    "List films per category (create the dataframe called \"film_category\"), the table must have the following structure:\n",
    "\n",
    "1. film_id\n",
    "2. film_title\n",
    "3. film_description\n",
    "3. film_category\n",
    "4. film_rental_rate\n",
    "5. film_length\n",
    "6. film_rating\n",
    "\n",
    "and measure the query execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+--------+-----+------+------+\n",
      "|FID|              title|         description|category|price|length|rating|\n",
      "+---+-------------------+--------------------+--------+-----+------+------+\n",
      "| 19|       AMADEUS HOLY|A Emotional Displ...|  Action| 0.99|   113|    PG|\n",
      "| 21|    AMERICAN CIRCUS|A Insightful Dram...|  Action| 4.99|   129|     R|\n",
      "| 29| ANTITRUST TOMATOES|A Fateful Yarn of...|  Action| 2.99|   168| NC-17|\n",
      "| 38|      ARK RIDGEMONT|A Beautiful Yarn ...|  Action| 0.99|    68| NC-17|\n",
      "| 56|BAREFOOT MANCHURIAN|A Intrepid Story ...|  Action| 2.99|   129|     G|\n",
      "| 67|       BERETS AGENT|A Taut Saga of a ...|  Action| 2.99|    77| PG-13|\n",
      "| 97|     BRIDE INTRIGUE|A Epic Tale of a ...|  Action| 0.99|    56|     G|\n",
      "|105|     BULL SHAWSHANK|A Fanciful Drama ...|  Action| 0.99|   125| NC-17|\n",
      "|111|    CADDYSHACK JEDI|A Awe-Inspiring E...|  Action| 0.99|    52| NC-17|\n",
      "|115|    CAMPUS REMEMBER|A Astounding Dram...|  Action| 2.99|   167|     R|\n",
      "|126|  CASUALTIES ENCINO|A Insightful Yarn...|  Action| 4.99|   179|     G|\n",
      "|130|     CELEBRITY HORN|A Amazing Documen...|  Action| 0.99|   110| PG-13|\n",
      "|162|    CLUELESS BUCKET|A Taut Tale of a ...|  Action| 2.99|    95|     R|\n",
      "|194|        CROW GREASE|A Awe-Inspiring D...|  Action| 0.99|   104|    PG|\n",
      "|205|        DANCES NONE|A Insightful Refl...|  Action| 0.99|    58| NC-17|\n",
      "|210|       DARKO DORADO|A Stunning Reflec...|  Action| 4.99|   130| NC-17|\n",
      "|212|     DARN FORRESTER|A Fateful Story o...|  Action| 4.99|   185|     G|\n",
      "|229|       DEVIL DESIRE|A Beautiful Refle...|  Action| 4.99|    87|     R|\n",
      "|250|       DRAGON SQUAD|A Taut Reflection...|  Action| 0.99|   170| NC-17|\n",
      "|252|       DREAM PICKUP|A Epic Display of...|  Action| 2.99|   135|    PG|\n",
      "+---+-------------------+--------------------+--------+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time:  0.36573100090026855\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "query3 ='''\n",
    "        SELECT \n",
    "        film.film_id AS FID,\n",
    "        film.title AS title,\n",
    "        film.description AS description,\n",
    "        category.name AS category,\n",
    "        film.rental_rate AS price,\n",
    "        film.length AS length,\n",
    "        film.rating AS rating\n",
    "    FROM\n",
    "        category\n",
    "        LEFT JOIN film_category ON category.category_id = film_category.category_id\n",
    "        LEFT JOIN film ON film_category.film_id = film.film_id\n",
    "'''\n",
    "\n",
    "film_category=ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query3)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "start = time.time()\n",
    "film_category.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Actor - Film dataframe \n",
    "\n",
    "The database in mysql is incomplete. In fact, there is a lack of information on addresses, cities and countries. \n",
    "This information is present in the \"esame\" database and in the collections: \"denormalizedAddress\", \"actor\" and \"film_actor\" in MongoDB. \n",
    "\n",
    "___\n",
    "\n",
    "The following pictures depict the structure of a typical document withing the \"actor\" and \"film_actor\" collections, respectively.\n",
    "\n",
    "![Actor collection](figures/actor.png)\n",
    "\n",
    "___\n",
    "\n",
    "![Film_Actor collection](figures/film_actor.png)\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "**Join** (using a pipeline of `lookup`, `unwind`, `project`, `concat` and `sort`) the \"actor\" and \"film_actor\" collections and extract a Spark dataFrame with the following columns:\n",
    "\n",
    "1. actor_id\n",
    "2. film_id\n",
    "3. name i.e. \"first_name last_name\"\n",
    "\n",
    "sorted by film_id\n",
    "\n",
    "> as for MySQL extracted dataframe for performance reasons is recommended to execute in-database operations (filters, joins, concat..) within the database itself\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "* [`$lookup`](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/). It is equivalent to a Join in **MongoDB Query Language (MQL)**\n",
    "* [`$unwind`](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/). Deconstructs an array field from the input documents to output a document for each element.\n",
    "* [`$project`](https://docs.mongodb.com/manual/reference/operator/aggregation/project/). Passes along the documents with the requested fields to the next stage in the pipeline. The specified fields can be existing fields from the input documents or newly computed fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+\n",
      "|actor_id|film_id|            name|\n",
      "+--------+-------+----------------+\n",
      "|       1|      1|PENELOPE GUINESS|\n",
      "|      10|      1| CHRISTIAN GABLE|\n",
      "|      20|      1|   LUCILLE TRACY|\n",
      "|      30|      1|     SANDRA PECK|\n",
      "|      40|      1|     JOHNNY CAGE|\n",
      "|      53|      1|     MENA TEMPLE|\n",
      "|     108|      1|    WARREN NOLTE|\n",
      "|     162|      1|    OPRAH KILMER|\n",
      "|     188|      1|    ROCK DUKAKIS|\n",
      "|     198|      1|     MARY KEITEL|\n",
      "|      19|      2|     BOB FAWCETT|\n",
      "|      85|      2|MINNIE ZELLWEGER|\n",
      "|      90|      2|    SEAN GUINESS|\n",
      "|     160|      2|      CHRIS DEPP|\n",
      "|       2|      3|   NICK WAHLBERG|\n",
      "|      19|      3|     BOB FAWCETT|\n",
      "|      24|      3|  CAMERON STREEP|\n",
      "|      64|      3|   RAY JOHANSSON|\n",
      "|     123|      3|  JULIANNE DENCH|\n",
      "|      41|      4| JODIE DEGENERES|\n",
      "+--------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "queryFA = [\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'film_actor', \n",
    "            'localField': 'actor_id', \n",
    "            'foreignField': 'actor_id', \n",
    "            'as': 'film'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': {\n",
    "            'path': '$film'\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 0, \n",
    "            'actor_id': 1, \n",
    "            'film_id': '$film.film_id', \n",
    "            'name': {\n",
    "                '$concat': [\n",
    "                    '$first_name', ' ', '$last_name'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$sort': {\n",
    "            'film_id': 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "FA = ss.read.format(\"mongo\")\\\n",
    ".option(\"pipeline\", queryFA)\\\n",
    ".option(\"uri\",\"mongodb://root:example@mongo/esame.actor?authSource=admin&readPreference=primaryPreferred\")\\\n",
    ".load()\n",
    "\n",
    "FA.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Actor - Film - Category dataframe\n",
    "\n",
    "Using the dataframe created in the previous cell (\"FA\") add to the \"film_category\" dataframe a column with the names (separated by commas) of the actors starring in each film.\n",
    "\n",
    "> hint: you might want to use **concat_ws** and **collect_list** function to aggregate the actors' nane\n",
    "\n",
    "Note that in the code snippet below film_category and FA are registered as temporary relational tables, this means that we can usa Spark SQL API to manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+-----------+-----+------+------+-------+--------------------+\n",
      "|FID|              title|         description|   category|price|length|rating|film_id|               actor|\n",
      "+---+-------------------+--------------------+-----------+-----+------+------+-------+--------------------+\n",
      "|148|     CHOCOLATE DUCK|A Unbelieveable S...|    Foreign| 2.99|   132|     R|    148|JOE SWANK, CAMERO...|\n",
      "|463|   INSTINCT AIRPORT|A Touching Docume...|     Sports| 2.99|   116|    PG|    463|JENNIFER DAVIS, G...|\n",
      "|471|    ISLAND EXORCIST|A Fanciful Panora...|   Classics| 2.99|    84| NC-17|    471|RAY JOHANSSON, AN...|\n",
      "|496|      KICK SAVANNAH|A Emotional Drama...|     Travel| 0.99|   179| PG-13|    496|ANGELA HUDSON, JE...|\n",
      "|833|    SPLENDOR PATTON|A Taut Story of a...|   Children| 0.99|   134|     R|    833|UMA WOOD, SANDRA ...|\n",
      "|243|    DOORS PRESIDENT|A Awe-Inspiring D...|  Animation| 4.99|    49| NC-17|    243|KARL BERRY, LUCIL...|\n",
      "|392|       HALL CASSIDY|A Beautiful Panor...|   Children| 4.99|    51| NC-17|    392|JOHNNY LOLLOBRIGI...|\n",
      "|540|       LUCKY FLYING|A Lacklusture Cha...|      Music| 2.99|    97| PG-13|    540|NICK WAHLBERG, KA...|\n",
      "|623|   NEWTON LABYRINTH|A Intrepid Charac...|    Foreign| 0.99|    75|    PG|    623|SEAN WILLIAMS, GR...|\n",
      "|737|      ROCK INSTINCT|A Astounding Char...|     Horror| 0.99|   102|     G|    737|MILLA PECK, JOHNN...|\n",
      "|858|      SUBMARINE BED|A Amazing Display...|     Comedy| 4.99|   127|     R|    858|JENNIFER DAVIS, P...|\n",
      "|897|       TORQUE BOUND|A Emotional Displ...|      Drama| 4.99|   179|     G|    897|UMA WOOD, NATALIE...|\n",
      "| 31|      APACHE DIVINE|A Awe-Inspiring R...|     Family| 4.99|    92| NC-17|     31|NICK WAHLBERG, CU...|\n",
      "|516|        LEGEND JEDI|A Awe-Inspiring E...|      Music| 0.99|    59|    PG|    516|TOM MCKELLEN, ANG...|\n",
      "| 85|   BONNIE HOLOCAUST|A Fast-Paced Stor...|Documentary| 0.99|    63|     G|     85|JOHNNY LOLLOBRIGI...|\n",
      "|137|     CHARADE DUFFEL|A Action-Packed D...|     Sci-Fi| 2.99|    66|    PG|    137|GREG CHAPLIN, KIR...|\n",
      "|251|DRAGONFLY STRANGERS|A Boring Document...|        New| 4.99|   133| NC-17|    251|CHRISTIAN GABLE, ...|\n",
      "|451|         IGBY MAKER|A Epic Documentar...|     Travel| 4.99|   160| NC-17|    451|BETTE NICHOLSON, ...|\n",
      "|580|        MINE TITANS|A Amazing Yarn of...|        New| 4.99|   166| PG-13|    580|NICK DEGENERES, H...|\n",
      "|808|         SLING LUKE|A Intrepid Charac...|   Classics| 0.99|    84|     R|    808|DAN TORN, GREG CH...|\n",
      "+---+-------------------+--------------------+-----------+-----+------+------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "film_category.registerTempTable('film_category')\n",
    "FA.registerTempTable('FA')\n",
    "\n",
    "full_film_category = ss.sql('''\n",
    "select * from film_category\n",
    "join \n",
    "(Select\n",
    "film_id,\n",
    "concat_ws(', ', collect_list(name)) as actor\n",
    "from FA\n",
    "group by film_id\n",
    ")as inSel \n",
    "on inSel.film_id =film_category.FID\n",
    "''')\n",
    "\n",
    "full_film_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclude the pipeline saving results as a parquet file\n",
    "\n",
    "Save the dataframe created in the previous cell as parquet file named \"film_category\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# put here the code\n",
    "full_film_category.write.mode('overwrite').parquet('film_category.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We implemented in Apache Spark our first Big Data Pipeline. Follows a grafical representation of the pipeline at issue.\n",
    "\n",
    "![Pipeline 1](figures/pipeline1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. More on indexing - Geospatial indexes in MongoDB\n",
    "\n",
    "**Create** a geospatial index (type 2dsphere) in mongodb on the \"location.coordinates\" field of the \"denormalizedAddress\" collection.\n",
    "\n",
    "Follows a picture with the typical structure of an address in \"denormalizedAddress\" collection:\n",
    "\n",
    "![denormalizedAddress collection](figures/address.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/a3/eb6f1fe5299ba556be2c7d3d79bd6a387c6a8adf7246967eabf7eada9287/pymongo-3.12.1-cp37-cp37m-manylinux2014_x86_64.whl (527kB)\n",
      "\u001b[K     |████████████████████████████████| 532kB 16.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.12.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'geo_index'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8\n",
    "# put here the code\n",
    "!pip install pymongo\n",
    "import pymongo\n",
    "import urllib.parse\n",
    "username = urllib.parse.quote_plus('root')\n",
    "password = urllib.parse.quote_plus('example')\n",
    "client = pymongo.MongoClient(\"mongodb://%s:%s@mongo:27017\" % (username, password))\n",
    "dblist = client.list_database_names()\n",
    "db =client.esame;\n",
    "db.denormalizedAddress.create_index([('location.coordinates', pymongo.GEOSPHERE)],name='geo_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Geospatial querying data from MongoDB to Spark\n",
    "\n",
    "**Retrive** all documents that contains coordinates within 200km from the point [ 8.659, 45.955 ]\n",
    "\n",
    "> you might want to use the **geoNear** aggregation\n",
    "\n",
    "**Select** only the following field: address_id, address, city_name, district, country_name\n",
    "\n",
    "Note that in the snipped below the selection part is performed using the Spark dataframe API, it may seems a waste of time but Spark uses the mongodb connector to push down the projection operator. In this way Spark does not load all data and then selects the columns; in fact, the querying and selection happens in an optimized way within MongoDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-----------+-------------+\n",
      "|address_id|             address|  city_name|   district| country_name|\n",
      "+----------+--------------------+-----------+-----------+-------------+\n",
      "|       444|231 Kaliningrad P...|    Bergamo|  Lombardia|        Italy|\n",
      "|        37|127 Purnea (Purni...|Alessandria|   Piemonte|        Italy|\n",
      "|       314|1224 Huejutla de ...|    Brescia|  Lombardia|        Italy|\n",
      "|       159|  185 Novi Sad Place|       Bern|       Bern|  Switzerland|\n",
      "|        61|    943 Tokat Street|      Vaduz|      Vaduz|Liechtenstein|\n",
      "|       604| 1331 Usak Boulevard|   Lausanne|       Vaud|  Switzerland|\n",
      "|        65|     915 Ponce Place|      Basel|Basel-Stadt|  Switzerland|\n",
      "+----------+--------------------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "\n",
    "distPip =[\n",
    "            {\n",
    "        '$geoNear': {\n",
    "            'near': {\n",
    "                'type': 'Point', \n",
    "                'coordinates': [\n",
    "                    8.659, 45.955\n",
    "                ]\n",
    "            }, \n",
    "            'distanceField': 'dist.calculated', \n",
    "            'maxDistance': 200000\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "disDF = ss.read.format(\"mongo\")\\\n",
    ".option(\"pipeline\", distPip)\\\n",
    ".option(\"uri\",\"mongodb://root:example@mongo/esame.denormalizedAddress?authSource=admin&readPreference=primaryPreferred\")\\\n",
    ".load().select('address_id', 'address', 'city_name', 'district', 'country_name')\n",
    "\n",
    "disDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Customers within a certain area\n",
    "\n",
    "Generate and a table with information about the customers whose address has been identified in the previous cell\n",
    "\n",
    "The table must have he following structure :\n",
    "\n",
    "1. Customer name (\"first_name last_name\")\n",
    "2. Customer address (\"address, city_name\")\n",
    "3. district, \n",
    "4. country_name\n",
    "5. active\n",
    "\n",
    "The information about the customers can be found in the \"customer.parquet\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----------+-------------+------+\n",
      "|             name|             address|   district| country_name|active|\n",
      "+-----------------+--------------------+-----------+-------------+------+\n",
      "|ALEXANDER FENNELL|231 Kaliningrad P...|  Lombardia|        Italy|  true|\n",
      "|        ANNA HILL|127 Purnea (Purni...|   Piemonte|        Italy|  true|\n",
      "|CHRISTOPHER GRECO|1224 Huejutla de ...|  Lombardia|        Italy|  true|\n",
      "|      GAIL KNIGHT|185 Novi Sad Plac...|       Bern|  Switzerland|  true|\n",
      "|    EVELYN MORGAN|943 Tokat Street,...|      Vaduz|Liechtenstein|  true|\n",
      "|    WADE DELVALLE|1331 Usak Bouleva...|       Vaud|  Switzerland|  true|\n",
      "| KATHERINE RIVERA|915 Ponce Place,B...|Basel-Stadt|  Switzerland|  true|\n",
      "+-----------------+--------------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. \n",
    "\n",
    "customers = ss.read.parquet(\"customer.parquet\")\n",
    "customers.registerTempTable('customer')\n",
    "disDF.registerTempTable('address')\n",
    "customers_full = ss.sql('''\n",
    "select \n",
    "concat(first_name,' ',last_name) AS name,\n",
    "concat(address,',',city_name) as address,\n",
    "district,\n",
    "country_name,\n",
    "active\n",
    "from customer natural join address\n",
    "''')\n",
    "customers_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Conclude the pipeline saving results as a parquet file\n",
    "\n",
    "Save the dataframe created in the previous cell as parquet file named \"custormer_full\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_full.write.mode('overwrite').parquet('custormer_full.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code concludes the second Spark-powered pipeline, representable as:\n",
    "![Pipeline 2](figures/pipeline2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
