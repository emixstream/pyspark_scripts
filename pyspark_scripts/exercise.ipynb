{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Data Pipeline with Apache Spark - Demo \n",
    "\n",
    "In this demo we will see together how to leverage [Apache Spark](https://spark.apache.org/) via Python ([PySpark](http://spark.apache.org/docs/latest/api/python/)) to process data from different sources representing a Data Lake. \n",
    "For the demo purpose we will use data stored in:\n",
    "\n",
    "* __MySQL__ - RDBMS database\n",
    "* __MongoDB__ - NoSQL database\n",
    "* __Parquet__ Files - [Apache Parquet](https://parquet.apache.org/) is a columnar data format often used in [Apache Hadoop](https://hadoop.apache.org/) environments, particularly suitable for analytics\n",
    "\n",
    "\n",
    "The basic idea is to design and implement a Big Data Pipeline consisting of several **Jobs** and **Tasks**. \n",
    "\n",
    "* Job: a complete data tranformation activity, from reading data from a source to saving them somewhere\n",
    "* Task: a single step of a job\n",
    "___\n",
    "\n",
    "To start the demo, run:\n",
    "\n",
    "`docker-compose -f ./docker-compose-full.yml up -d`\n",
    " \n",
    "To stop the demo, run:\n",
    "\n",
    "`docker-compose -f ./docker-compose-full.yml down`\n",
    "\n",
    "Docker will build up an ecosystem with:\n",
    "\n",
    "* **MongoDB** with a populated database\n",
    "* **MySQL** with a populated database\n",
    "* **Apache Spark** deployed in Standalone Mode\n",
    "* **Jupyter** enabled to work with Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "In the first cell we have to instantiate the __SparkSession__ object. Via the SparkSession we can read, manipulate, and store data from different data sources using both RDD and DataFrame API. \n",
    "\n",
    "> Only one SparkSession object can be contained in a Spark-powered program. The SparkSession creates and handles the DAG and interacts with the exectutors to execute it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://root:example@mongo/test.coll?authSource=admin\") \\\n",
    ".config(\"spark.mongodb.output.uri\", \"mongodb://root:example@mongo/test.coll?authSource=admin\") \\\n",
    ".config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.17,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1') \\\n",
    ".getOrCreate()\n",
    "ss.version\n",
    "# Spark version 2.4.4 uses Scala 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [builder pattern](https://en.wikipedia.org/wiki/Builder_pattern) is used to create and initialize the SparkSession. Notice that we used the *config* method to store metadata as the MondoDB input and output uris and a list of java packages (fully specified in [Apache Maven](https://maven.apache.org/) format). \n",
    "\n",
    "> Note that Spark checks if the listed jar packages are available at executor level, otherwise it downloads them.  \n",
    "\n",
    "Other metadata (hostname, username, password, etc.) are to be expressed as python variable. Next cell reports MySQL connection parameters. In the following the **jdbcUrl** is used for DataFrame creation from MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"mysql\"\n",
    "jdbcDatabase = \"esame\"\n",
    "username = \"root\"\n",
    "password = \"example\"\n",
    "jdbcPort = 3306\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Total sales per film category\n",
    "\n",
    "List the **total sales per film category** considering only the sales referred to rented movies.\n",
    "Measure the query execution time (you might want to use the python 'time' library)\n",
    "\n",
    "You may need to use the following tables: \n",
    "\n",
    "1. category\n",
    "2. film_category \n",
    "3. inventory \n",
    "3. payment \n",
    "4. rental\n",
    "\n",
    "![EER Diagram](figures/mysql_table1.png)\n",
    "\n",
    "\n",
    "The result must have __two columns__:\n",
    "\n",
    "1. the film_category\n",
    "2. the total_sales\n",
    "\n",
    "and has to be **sorted in descending order** with respect to the **total_sales**.\n",
    "\n",
    "> For performance reasons it is recommended to write a SQL query rather than import the data in different spark dataframes and use spark to join them. It is better because Spark is able to communicate with MySQL and push down filter and join operations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet presents how to create a dataframe from MySQL. \n",
    "\n",
    "Note that it is necessary to provide:\n",
    "\n",
    "1. The protocol to use (jdbc, driver)\n",
    "2. The connection string (url)\n",
    "3. The query MySQL has to execute to export data to Spark\n",
    "\n",
    "Moreover, note that the dataframe API is lazy evaluated, it needs an **action**. In the code snippet below:\n",
    "\n",
    "* load() is not an action, the dataframe is therefore only defined but not created\n",
    "* show() is an action, the dataframe is created here and the results are returned to the main program (driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "import time\n",
    "\n",
    "query1 =  '''\n",
    "        XXXX put your query here\n",
    "    '''\n",
    "\n",
    "salesCat = ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query1)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "salesCat.show() # this is an action, the dataframe is created only at this point.\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimizing data loading with indexes\n",
    "\n",
    "Indexes are extremely important for speeding up analytical processes, particularly when JOIN operations must be performed. It is easy to implement indexes in RDBMS and NoSQL systems, a little less easy when dealing with large files in HDFS. \n",
    "\n",
    "**Optimize** the query created in the previous cell by entering the appropriate indexes in the mysql database. \n",
    "\n",
    "**Report** as comments the sql statements used to create the indexes\n",
    "\n",
    "**Re-execute** the query (reating the dataframe df2) and measure the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "start = time.time()\n",
    "salesCat.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing data loading with views\n",
    "\n",
    "Views are a great way to simplify the definition of a data pipeline because it defines tasks to be executed at the database level, in some cases it also allows to improve the overall query performance. \n",
    "\n",
    "In the big data world, a view can be implemented as a **batch process**, for example implemented through [Apache HIVE](https://hive.apache.org/), which makes partially pre-processed data available. \n",
    "This is very useful when you want to crate a data pipeline for production, a bit less so when you want to implement exploratory actions.\n",
    "\n",
    "1. **Create** a view called \"total_sales\" from the query implemented in the previous cells.\n",
    "2. **Report** here the sql statement used to create the view\n",
    "3. **Load** in a spark dataframe all the rows of the view and show them\n",
    "4. **Measure** the data loading time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "# put here the sql statement for the view creation\n",
    "\n",
    "import time\n",
    "\n",
    "query2 = '''XXX here the query for loading all data from total_sales view'''\n",
    "\n",
    "dfview = ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query2)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "start = time.time()\n",
    "dfview.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Films per category\n",
    "\n",
    "List films per category (create the dataframe called \"film_category\"), the table must have the following structure:\n",
    "\n",
    "1. film_id\n",
    "2. film_title\n",
    "3. film_description\n",
    "3. film_category\n",
    "4. film_rental_rate\n",
    "5. film_length\n",
    "6. film_rating\n",
    "\n",
    "and measure the query execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "query3 ='''\n",
    "    XXX put the query here\n",
    "'''\n",
    "\n",
    "film_category=ss.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"query\", query3)\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "start = time.time()\n",
    "film_category.show()\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Actor - Film dataframe \n",
    "\n",
    "The database in mysql is incomplete. In fact, there is a lack of information on addresses, cities and countries. \n",
    "This information is present in the \"esame\" database and in the collections: \"denormalizedAddress\", \"actor\" and \"film_actor\" in MongoDB. \n",
    "\n",
    "___\n",
    "\n",
    "The following pictures depict the structure of a typical document withing the \"actor\" and \"film_actor\" collections, respectively.\n",
    "\n",
    "![Actor collection](figures/actor.png)\n",
    "\n",
    "___\n",
    "\n",
    "![Film_Actor collection](figures/film_actor.png)\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "**Join** (using a pipeline of `lookup`, `unwind`, `project`, `concat` and `sort`) the \"actor\" and \"film_actor\" collections and extract a Spark dataFrame with the following columns:\n",
    "\n",
    "1. actor_id\n",
    "2. film_id\n",
    "3. name i.e. \"first_name last_name\"\n",
    "\n",
    "sorted by film_id\n",
    "\n",
    "> as for MySQL extracted dataframe for performance reasons is recommended to execute in-database operations (filters, joins, concat..) within the database itself\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "* [`$lookup`](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/). It is equivalent to a Join in **MongoDB Query Language (MQL)**\n",
    "* [`$unwind`](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/). Deconstructs an array field from the input documents to output a document for each element.\n",
    "* [`$project`](https://docs.mongodb.com/manual/reference/operator/aggregation/project/). Passes along the documents with the requested fields to the next stage in the pipeline. The specified fields can be existing fields from the input documents or newly computed fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "queryFA = [\n",
    "            XXX put the mongo pipeline here\n",
    "]\n",
    "\n",
    "FA = ss.read.format(\"mongo\")\\\n",
    ".option(\"pipeline\", queryFA)\\\n",
    ".option(\"uri\",\"mongodb://root:example@mongo/esame.actor?authSource=admin&readPreference=primaryPreferred\")\\\n",
    ".load()\n",
    "\n",
    "FA.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Actor - Film - Category dataframe\n",
    "\n",
    "Using the dataframe created in the previous cell (\"FA\") add to the \"film_category\" dataframe a column with the names (separated by commas) of the actors starring in each film.\n",
    "\n",
    "> hint: you might want to use **concat_ws** and **collect_list** function to aggregate the actors' nane\n",
    "\n",
    "Note that in the code snippet below film_category and FA are registered as temporary relational tables, this means that we can usa Spark SQL API to manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "\n",
    "film_category.registerTempTable('film_category')\n",
    "FA.registerTempTable('FA')\n",
    "\n",
    "full_film_category = ss.sql('''\n",
    "        XXX put the query here\n",
    "''')\n",
    "\n",
    "full_film_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclude the pipeline saving results as a parquet file\n",
    "\n",
    "Save the dataframe created in the previous cell as parquet file named \"film_category\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# put here the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We implemented in Apache Spark our first Big Data Pipeline. Follows a grafical representation of the pipeline at issue.\n",
    "\n",
    "![Pipeline 1](figures/pipeline1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. More on indexing - Geospatial indexes in MongoDB\n",
    "\n",
    "**Create** a geospatial index (type 2dsphere) in mongodb on the \"location.coordinates\" field of the \"denormalizedAddress\" collection.\n",
    "\n",
    "Follows a picture with the typical structure of an address in \"denormalizedAddress\" collection:\n",
    "\n",
    "![denormalizedAddress collection](figures/address.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "# put here the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Geospatial querying data from MongoDB to Spark\n",
    "\n",
    "**Retrive** all documents that contains coordinates within 200km from the point [ 8.659, 45.955 ]\n",
    "\n",
    "> you might want to use the **geoNear** aggregation\n",
    "\n",
    "**Select** only the following field: address_id, address, city_name, district, country_name\n",
    "\n",
    "Note that in the snipped below the selection part is performed using the Spark dataframe API, it may seems a waste of time but Spark uses the mongodb connector to push down the projection operator. In this way Spark does not load all data and then selects the columns; in fact, the querying and selection happens in an optimized way within MongoDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "\n",
    "distPip =[\n",
    "        XXX put the geo-spatial query here\n",
    "]\n",
    "\n",
    "disDF = ss.read.format(\"mongo\")\\\n",
    ".option(\"pipeline\", distPip)\\\n",
    ".option(\"uri\",\"mongodb://root:example@mongo/esame.denormalizedAddress?authSource=admin&readPreference=primaryPreferred\")\\\n",
    ".load().select('address_id', 'address', 'city_name', 'district', 'country_name')\n",
    "\n",
    "disDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Customers within a certain area\n",
    "\n",
    "Generate and a table with information about the customers whose address has been identified in the previous cell\n",
    "\n",
    "The table must have he following structure :\n",
    "\n",
    "1. Customer name (\"first_name last_name\")\n",
    "2. Customer address (\"address, city_name\")\n",
    "3. district, \n",
    "4. country_name\n",
    "5. active\n",
    "\n",
    "The information about the customers can be found in the \"customer.parquet\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. \n",
    "\n",
    "customers = ss.read.parquet(\"customer.parquet\")\n",
    "customers.registerTempTable('customer')\n",
    "disDF.registerTempTable('address')\n",
    "ss.sql('''\n",
    "XXX put the query here\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Conclude the pipeline saving results as a parquet file\n",
    "\n",
    "Save the dataframe created in the previous cell as parquet file named \"Custormer_full\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code concludes the second Spark-powered pipeline, representable as:\n",
    "![Pipeline 2](figures/pipeline2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
